import argparse
import random
import torch

import numpy as np
import torch.nn.functional as F

from torch import nn
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import GPT2Tokenizer
from einops import rearrange

from datasets import (
  SonnetsDataset,
)
from models.gpt2 import GPT2Model

from optimizer import AdamW

TQDM_DISABLE = False

# ì¬í˜„ì„±ì„ ìœ„í•œ random seed ê³ ì •.
def seed_everything(seed=11711):
  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  torch.cuda.manual_seed_all(seed)
  torch.backends.cudnn.benchmark = False
  torch.backends.cudnn.deterministic = True


class SonnetGPT(nn.Module):
  """Prefix-Tuningê³¼ Unlikelihood Lossë¥¼ ê²°í•©í•œ ì†Œë„¤íŠ¸ ìƒì„± ëª¨ë¸"""

  def __init__(self, args):
    super().__init__()
    self.gpt = GPT2Model.from_pretrained(model=args.model_size, d=args.d, l=args.l, num_heads=args.num_heads)
    self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    self.tokenizer.pad_token = self.tokenizer.eos_token

    # Prefix-Tuning ì„¤ì •
    self.prefix_length = args.prefix_length
    self.prefix_dim = args.d  # ëª¨ë¸ì˜ hidden dimensionê³¼ ê°™ê²Œ
    
    # í•™ìŠµ ê°€ëŠ¥í•œ prefix ì„ë² ë”© (virtual tokens)
    self.prefix_embeddings = nn.Parameter(
        torch.randn(self.prefix_length, self.prefix_dim) * 0.1
    )
    
    # Prefixë¥¼ hidden statesì— ì˜í–¥ì„ ì£¼ëŠ” ê°„ë‹¨í•œ MLP
    self.prefix_mlp = nn.Sequential(
        nn.Linear(self.prefix_dim, self.prefix_dim * 2),
        nn.Tanh(),
        nn.Linear(self.prefix_dim * 2, self.prefix_dim)  # ìµœì¢… ì¶œë ¥ì€ hidden_dimê³¼ ê°™ê²Œ
    )

    # Unlikelihood Loss ê´€ë ¨ íŒŒë¼ë¯¸í„°
    self.alpha = args.alpha if hasattr(args, 'alpha') else 1.0  # Unlikelihood loss weight
    self.repetition_window = args.repetition_window if hasattr(args, 'repetition_window') else 5
    
    # ë§ˆì§€ë§‰ ì¼ë¶€ ë ˆì´ì–´ë§Œ fine-tuning (ì˜ˆ: ë§ˆì§€ë§‰ 4ê°œ ë ˆì´ì–´)
    for param in self.gpt.parameters():
      param.requires_grad = True

    # ì¼ë¶€ ë ˆì´ì–´ë§Œ fine-tuning (ë§ˆì§€ë§‰ 4ê°œë§Œ)
    num_layers = len(self.gpt.gpt_layers)
    for i, block in enumerate(self.gpt.gpt_layers):
      for param in block.parameters():
        param.requires_grad = i >= (num_layers - 4)

  def get_prefix_influence(self, batch_size, seq_len):
    """prefix embeddingsì˜ ì˜í–¥ì„ hidden statesì— ë°˜ì˜"""
    # prefix_embeddingsë¥¼ MLPë¥¼ í†µí•´ ë³€í™˜
    prefix_tokens = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # [B, prefix_length, prefix_dim]
    prefix_influence = self.prefix_mlp(prefix_tokens)  # [B, prefix_length, hidden_dim]
    
    # í‰ê· ì„ ë‚´ì–´ ì „ì²´ sequenceì— ì ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
    prefix_influence = prefix_influence.mean(dim=1, keepdim=True)  # [B, 1, hidden_dim]
    prefix_influence = prefix_influence.expand(-1, seq_len, -1)  # [B, seq_len, hidden_dim]
    
    return prefix_influence * 0.1  # ì‘ì€ ì˜í–¥ë ¥ìœ¼ë¡œ ì‹œì‘

  def forward(self, input_ids, attention_mask, use_prefix=True):
    batch_size = input_ids.size(0)
    
    if use_prefix:
      # Prefix-Tuning: prefix embeddingsë¥¼ hidden statesì— ì¶”ê°€í•˜ì—¬ ì˜í–¥ ì£¼ê¸°
      outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)
      hidden_states = outputs["last_hidden_state"]  # shape: [batch_size, seq_len, hidden_dim]
      
      # prefix influenceë¥¼ hidden statesì— ì¶”ê°€
      prefix_influence = self.get_prefix_influence(batch_size, hidden_states.size(1))
      hidden_states = hidden_states + prefix_influence
      
    else:
      outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)
      hidden_states = outputs["last_hidden_state"]  # shape: [batch_size, seq_len, hidden_dim]

    # hidden statesë¥¼ vocabulary logitìœ¼ë¡œ ë³€í™˜
    logits = self.gpt.hidden_state_to_token(hidden_states)  # shape: [batch_size, seq_len, vocab_size]
    return logits

  def compute_unlikelihood_loss(self, logits, input_ids):
    """Unlikelihood Loss ê³„ì‚° í•¨ìˆ˜ - ë°˜ë³µë˜ëŠ” í† í°ì˜ í™•ë¥ ì„ ë‚®ì¶¤"""
    batch_size, seq_len, vocab_size = logits.shape
    total_ul_loss = 0.0
    
    for b in range(batch_size):
      for t in range(1, seq_len):  # ì²« ë²ˆì§¸ í† í°ì€ ìŠ¤í‚µ
        current_token = input_ids[b, t]
        
        # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê³¼ê±° repetition_window ë‚´ì˜ í† í°ë“¤ í™•ì¸
        start_idx = max(0, t - self.repetition_window)
        context_tokens = input_ids[b, start_idx:t]
        
        # ë°˜ë³µë˜ëŠ” í† í°ë“¤ ì°¾ê¸°
        repeated_tokens = []
        for prev_token in context_tokens:
          if prev_token == current_token:
            repeated_tokens.append(prev_token.item())
        
        # ë°˜ë³µëœ í† í°ë“¤ì— ëŒ€í•´ unlikelihood loss ì ìš©
        if repeated_tokens:
          current_logits = logits[b, t]  # [vocab_size]
          probs = torch.softmax(current_logits, dim=-1)
          
          # ë°˜ë³µëœ í† í°ë“¤ì˜ í™•ë¥ ì„ ë‚®ì¶”ëŠ” ì†ì‹¤
          for token_id in set(repeated_tokens):  # ì¤‘ë³µ ì œê±°
            if token_id < vocab_size:
              # -log(1 - p(token)) : í™•ë¥ ì´ ë†’ì„ìˆ˜ë¡ ì†ì‹¤ì´ ì»¤ì§
              token_prob = probs[token_id]
              ul_loss = -torch.log(torch.clamp(1.0 - token_prob, min=1e-8))
              total_ul_loss += ul_loss
    
    return total_ul_loss / (batch_size * seq_len)  # ì •ê·œí™”

  def compute_combined_loss(self, logits, labels, input_ids):
    """Standard Cross-Entropy + Unlikelihood Loss"""
    # ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸ë§ ì†ì‹¤
    ce_logits = rearrange(logits[:, :-1], 'b t d -> (b t) d')
    ce_labels = labels[:, 1:].contiguous().flatten()
    ce_loss = F.cross_entropy(ce_logits, ce_labels, reduction='mean')
    
    # Unlikelihood ì†ì‹¤
    ul_loss = self.compute_unlikelihood_loss(logits, input_ids)
    
    # ê²°í•©ëœ ì†ì‹¤
    total_loss = ce_loss + self.alpha * ul_loss
    
    return total_loss, ce_loss, ul_loss

  def get_device(self):
    for param in self.gpt.parameters():
      return param.device

  @torch.no_grad()
  def generate(self, encoding, temperature=0.7, top_p=0.9, max_length=128, target_lines=14):
    """Prefix-Tuningì´ ì ìš©ëœ Top-p sampling ê¸°ë°˜ 14ì¤„ ì†Œë„¤íŠ¸ ìƒì„±"""
    input_ids = encoding.to(self.get_device())
    generated = input_ids.clone()
    
    # êµ¬ë‘ì  í† í° IDë“¤ (ë¬¸ì¥ ì¢…ë£Œë¥¼ ìœ„í•œ)
    punctuation_tokens = {
        self.tokenizer.encode('.')[0]: 2.0,    # ë§ˆì¹¨í‘œ
        self.tokenizer.encode(',')[0]: 1.5,    # ì‰¼í‘œ  
        self.tokenizer.encode(';')[0]: 1.5,    # ì„¸ë¯¸ì½œë¡ 
        self.tokenizer.encode(':')[0]: 1.3,    # ì½œë¡ 
        self.tokenizer.encode('!')[0]: 1.8,    # ëŠë‚Œí‘œ
        self.tokenizer.encode('?')[0]: 1.8,    # ë¬¼ìŒí‘œ
    }

    for step in range(max_length):
        # í˜„ì¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        decoded_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)
        current_lines = decoded_text.count('\n')

        # ğŸ¯ 14ì¤„ ì´ìƒì´ë©´ ì¤‘ë‹¨
        if current_lines >= target_lines:
            break

        # ëª¨ë¸ forward (prefix ì‚¬ìš©)
        attention_mask = torch.ones_like(generated).to(self.get_device())
        logits = self.forward(generated, attention_mask, use_prefix=True)[:, -1, :]  # [B, vocab]

        # ë¬¸ì¥ ê¸¸ì´ ì œì–´: í˜„ì¬ ì¤„ì˜ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        current_line_text = decoded_text.split('\n')[-1] if '\n' in decoded_text else decoded_text
        current_line_words = len(current_line_text.split())
        
        # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ êµ¬ë‘ì  ìƒì„±ì„ ì¥ë ¤
        if current_line_words > 8:  # 8ë‹¨ì–´ ì´ìƒì´ë©´ êµ¬ë‘ì  ì¥ë ¤
            punctuation_boost = min(2.0, (current_line_words - 8) * 0.3)  # ìµœëŒ€ 2ë°°ê¹Œì§€
            for token_id, base_boost in punctuation_tokens.items():
                if token_id < logits.size(-1):
                    logits[0, token_id] += punctuation_boost * base_boost
        
        # ë§¤ìš° ê¸´ ë¬¸ì¥ì˜ ê²½ìš° ì¤„ë°”ê¿ˆ ê°•ì œ ì¥ë ¤
        if current_line_words > 12:
            newline_token = self.tokenizer.encode('\n')[0] if self.tokenizer.encode('\n') else None
            if newline_token and newline_token < logits.size(-1):
                logits[0, newline_token] += 3.0

        # ë°˜ë³µ ë°©ì§€: ìµœê·¼ ìƒì„±ëœ í† í°ë“¤ê³¼ ê°™ì€ í† í°ì˜ í™•ë¥ ì„ ë‚®ì¶¤
        if generated.size(1) > self.repetition_window:
            recent_tokens = generated[0, -self.repetition_window:].tolist()
            for token_id in set(recent_tokens):
                if token_id < logits.size(-1):
                    logits[0, token_id] -= 1.0  # ë°˜ë³µ í† í° í™•ë¥  ê°ì†Œ

        # Temperature ì ìš© ë° numerical stability ë³´ì¥
        logits = logits / max(temperature, 1e-8)
        
        # Top-p ìƒ˜í”Œë§
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        probs = torch.softmax(sorted_logits, dim=-1)
        cumulative_probs = probs.cumsum(dim=-1)

        # Top-p í•„í„°ë§
        sorted_mask = cumulative_probs > top_p
        # ìµœì†Œí•œ í•˜ë‚˜ì˜ í† í°ì€ ìœ ì§€í•˜ë„ë¡ ë³´ì¥
        if sorted_mask.all():
            sorted_mask[0] = False
        
        # ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ë¥¼ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •
        sorted_logits = sorted_logits.clone()
        sorted_logits[sorted_mask] = sorted_logits[sorted_mask] - 1e10
        
        # ë‹¤ì‹œ softmax ì ìš©
        probs = torch.softmax(sorted_logits, dim=-1)
        
        # Numerical stability ì²´í¬
        if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():
            # ë¬¸ì œê°€ ìˆìœ¼ë©´ uniform distributionìœ¼ë¡œ fallback
            probs = torch.ones_like(probs) / probs.size(-1)
        
        # í™•ë¥  ì •ê·œí™”
        probs = probs / probs.sum(dim=-1, keepdim=True)
        
        # ìƒ˜í”Œë§
        try:
            next_token = torch.multinomial(probs, num_samples=1)
            next_token_id = sorted_indices.gather(-1, next_token)
        except RuntimeError:
            # multinomialì´ ì‹¤íŒ¨í•˜ë©´ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ
            next_token = torch.argmax(probs, dim=-1, keepdim=True)
            next_token_id = sorted_indices.gather(-1, next_token)

        # ë‹¤ìŒ í† í° ë¶™ì´ê¸°
        generated = torch.cat([generated, next_token_id], dim=1)

    # ë””ì½”ë”© í›„ ì •í™•íˆ 14ì¤„ë¡œ ì œí•œ
    decoded_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)
    lines = decoded_text.strip().split('\n')
    trimmed_text = '\n'.join(lines[:target_lines])

    return generated, trimmed_text


def save_model(model, optimizer, args, filepath):
  save_info = {
    'model': model.state_dict(),
    'optim': optimizer.state_dict(),
    'args': args,
    'system_rng': random.getstate(),
    'numpy_rng': np.random.get_state(),
    'torch_rng': torch.random.get_rng_state(),
  }

  torch.save(save_info, filepath)
  print(f"save the model to {filepath}")


def train(args):
  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')
  sonnet_dataset = SonnetsDataset(args.sonnet_path)
  sonnet_dataloader = DataLoader(sonnet_dataset, shuffle=True, batch_size=args.batch_size,
                                 collate_fn=sonnet_dataset.collate_fn)
  held_out_sonnet_dataset = SonnetsDataset(args.held_out_sonnet_path)

  args = add_arguments(args)
  model = SonnetGPT(args).to(device)
  
  # Prefix-Tuning ëª¨ë“œ ì„¤ì •
  if args.prefix_only:
    # GPT íŒŒë¼ë¯¸í„° ê³ ì •, Prefix ê´€ë ¨ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
    for param in model.gpt.parameters():
      param.requires_grad = False
    
    trainable_params = [model.prefix_embeddings]
    for param in model.prefix_mlp.parameters():
      trainable_params.append(param)
    
    optimizer = AdamW(trainable_params, lr=args.lr)
    print("Training only prefix parameters (Prefix-Tuning mode)")
  else:
    # ê¸°ì¡´ ë°©ì‹: ì¼ë¶€ ë ˆì´ì–´ + prefix parameters í•™ìŠµ
    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)
    print("Training last layers + prefix parameters (Hybrid mode)")

  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)

  best_loss = float('inf')
  patience = 5
  counter = 0

  for epoch in range(args.epochs):
    model.train()
    train_loss = 0
    ce_loss_sum = 0
    ul_loss_sum = 0
    num_batches = 0

    for batch in tqdm(sonnet_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):
        b_ids, b_mask = batch['token_ids'].to(device), batch['attention_mask'].to(device)
        optimizer.zero_grad()
        
        # Forward pass with Prefix-Tuning
        logits = model(b_ids, b_mask, use_prefix=True)
        
        # Combined Loss: Cross-Entropy + Unlikelihood Loss
        total_loss, ce_loss, ul_loss = model.compute_combined_loss(logits, b_ids, b_ids)
        
        # Backward pass
        total_loss.backward()
        optimizer.step()

        # í†µê³„ ìˆ˜ì§‘
        train_loss += total_loss.item()
        ce_loss_sum += ce_loss.item()
        ul_loss_sum += ul_loss.item()
        num_batches += 1

    # ì—í¬í¬ í†µê³„
    avg_train_loss = train_loss / num_batches
    avg_ce_loss = ce_loss_sum / num_batches
    avg_ul_loss = ul_loss_sum / num_batches
    
    scheduler.step(avg_train_loss)
    print(f"Epoch {epoch}: total :: {avg_train_loss:.3f} | CE :: {avg_ce_loss:.3f} | UL :: {avg_ul_loss:.3f}")

    if avg_train_loss < best_loss:
        best_loss = avg_train_loss
        counter = 0
        print(f"ğŸ’¾ Saving best model at epoch {epoch} with total loss {avg_train_loss:.3f}")
        save_model(model, optimizer, args, f'best_{args.filepath}')
    else:
        counter += 1
        if counter >= patience:
            print("â¹ï¸ Early stopping triggered.")
            break

    # ë§¤ ì—í¬í¬ë§ˆë‹¤ ìƒ˜í”Œ ìƒì„± (ëª¨ë‹ˆí„°ë§)
    if epoch % 2 == 0:
        print('ğŸ­ Generating sample sonnets...')
        model.eval()
        sample_count = 0
        for batch in held_out_sonnet_dataset:
            if sample_count >= 2:
                break
            encoding = model.tokenizer(batch[1], return_tensors='pt', padding=True, truncation=True)
            encoding = {k: v.to(device) for k, v in encoding.items()}
            output = model.generate(encoding['input_ids'], temperature=args.temperature, top_p=args.top_p)
            print(f'Sample {sample_count + 1}:')
            print(f'{batch[1]}{output[1]}\n')
            sample_count += 1
        
  print("ğŸ‰ Training completed!")
  return model

@torch.no_grad()
def generate_submission_sonnets(args):
  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')
  # ìµœì¢… í›ˆë ¨ëœ ëª¨ë¸ ë˜ëŠ” best ëª¨ë¸ ë¡œë“œ
  try:
    saved = torch.load(f'best_{args.filepath}', weights_only=False)
    print("ğŸ“‚ Loading best model for generation...")
  except FileNotFoundError:
    saved = torch.load(f'{args.epochs-1}_{args.filepath}', weights_only=False)
    print("ğŸ“‚ Loading final epoch model for generation...")
  
  model = SonnetGPT(saved['args'])
  model.load_state_dict(saved['model'])
  model = model.to(device).eval()

  held_out_sonnet_dataset = SonnetsDataset(args.held_out_sonnet_path)
  generated_sonnets = []

  print("ğŸ­ Generating submission sonnets with Prefix-Tuning + Unlikelihood Loss...")
  for batch in held_out_sonnet_dataset:
    sonnet_id = batch[0]
    encoding = model.tokenizer(batch[1], return_tensors='pt', padding=False, truncation=True)
    encoding = {k: v.to(device) for k, v in encoding.items()}
    output = model.generate(encoding['input_ids'], temperature=args.temperature, top_p=args.top_p)
    decoded_output = model.tokenizer.decode(output[0][0])
    generated_sonnets.append((sonnet_id, f'{decoded_output}\n\n'))

  with open(args.sonnet_out, "w+") as f:
    f.write(f"--Generated Sonnets (Prefix-Tuning + Unlikelihood Loss)-- \n\n")
    for sonnet in generated_sonnets:
      f.write(f"\n{sonnet[0]}\n")
      f.write(sonnet[1])
  
  print(f"âœ… Generated sonnets saved to {args.sonnet_out}")


def get_args():
  parser = argparse.ArgumentParser()
  parser.add_argument("--sonnet_path", type=str, default="data/sonnets.txt")
  parser.add_argument("--held_out_sonnet_path", type=str, default="data/sonnets_held_out.txt")
  parser.add_argument("--sonnet_out", type=str, default="predictions/generated_sonnets_C.txt")
  parser.add_argument("--seed", type=int, default=11711)
  parser.add_argument("--epochs", type=int, default=15)
  parser.add_argument("--use_gpu", action='store_true')
  parser.add_argument("--temperature", type=float, default=0.8)
  parser.add_argument("--top_p", type=float, default=0.9)
  parser.add_argument("--batch_size", type=int, default=8)
  parser.add_argument("--lr", type=float, default=1e-5)
  parser.add_argument("--model_size", type=str, choices=['gpt2', 'gpt2-medium', 'gpt2-large'], default='gpt2')
  
  # Prefix-Tuning ê´€ë ¨ íŒŒë¼ë¯¸í„°
  parser.add_argument("--prefix_length", type=int, default=10, help="Length of prefix tokens")
  parser.add_argument("--prefix_only", action='store_true', help="Train only prefix parameters (freeze GPT)")
  
  # Unlikelihood Loss ê´€ë ¨ íŒŒë¼ë¯¸í„°
  parser.add_argument("--alpha", type=float, default=1.0, help="Weight for unlikelihood loss")
  parser.add_argument("--repetition_window", type=int, default=5, help="Window size for detecting repetitions")
  
  return parser.parse_args()

def add_arguments(args):
  if args.model_size == 'gpt2':
    args.d = 768
    args.l = 12
    args.num_heads = 12
  elif args.model_size == 'gpt2-medium':
    args.d = 1024
    args.l = 24
    args.num_heads = 16
  elif args.model_size == 'gpt2-large':
    args.d = 1280
    args.l = 36
    args.num_heads = 20
  else:
    raise Exception(f'{args.model_size} is not supported.')
  return args

if __name__ == "__main__":
  args = get_args()
  args.filepath = f'{args.epochs}-{args.lr}-sonnet_C.pt'
  seed_everything(args.seed)

  # í›ˆë ¨ ì‹¤í–‰
  trained_model = train(args)
  
  # í›ˆë ¨ ì™„ë£Œ í›„ ìµœì¢… ì†Œë„¤íŠ¸ ìƒì„±
  print("\n" + "="*60)
  print("ğŸ­ Training completed! Generating final submission sonnets...")
  print("="*60)
  generate_submission_sonnets(args)