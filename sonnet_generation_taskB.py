import argparse
import random
import torch

import numpy as np
import torch.nn.functional as F

from torch import nn
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import GPT2Tokenizer
from einops import rearrange

from datasets import (
  SonnetsDataset,
)
from models.gpt2 import GPT2Model

from optimizer import AdamW

TQDM_DISABLE = False

# ì¬í˜„ì„±ì„ ìœ„í•œ random seed ê³ ì •.
def seed_everything(seed=11711):
  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  torch.cuda.manual_seed(seed)
  torch.cuda.manual_seed_all(seed)
  torch.backends.cudnn.benchmark = False
  torch.backends.cudnn.deterministic = True


class SonnetGPT(nn.Module):
  """Sonnet ìƒì„±ì„ ìœ„í•´ ì„¤ê³„ëœ ì—¬ëŸ¬ë¶„ì˜ GPT-2 ëª¨ë¸."""

  def __init__(self, args):
    super().__init__()
    self.gpt = GPT2Model.from_pretrained(model=args.model_size, d=args.d, l=args.l, num_heads=args.num_heads)
    self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    self.tokenizer.pad_token = self.tokenizer.eos_token

    # Prefix-Tuning ì„¤ì •
    self.prefix_length = args.prefix_length
    self.prefix_dim = args.d  # ëª¨ë¸ì˜ hidden dimensionê³¼ ê°™ê²Œ
    
    # í•™ìŠµ ê°€ëŠ¥í•œ prefix ì„ë² ë”© (virtual tokens)
    self.prefix_embeddings = nn.Parameter(
        torch.randn(self.prefix_length, self.prefix_dim) * 0.1
    )
    
    # Prefixë¥¼ hidden statesì— ì˜í–¥ì„ ì£¼ëŠ” ê°„ë‹¨í•œ MLP
    self.prefix_mlp = nn.Sequential(
        nn.Linear(self.prefix_dim, self.prefix_dim * 2),
        nn.Tanh(),
        nn.Linear(self.prefix_dim * 2, self.prefix_dim)  # ìµœì¢… ì¶œë ¥ì€ hidden_dimê³¼ ê°™ê²Œ
    )

    # ë§ˆì§€ë§‰ ì¼ë¶€ ë ˆì´ì–´ë§Œ fine-tuning (ì˜ˆ: ë§ˆì§€ë§‰ 4ê°œ ë ˆì´ì–´)
    for param in self.gpt.parameters():
      param.requires_grad = True

    # ì¼ë¶€ ë ˆì´ì–´ë§Œ fine-tuning (ë§ˆì§€ë§‰ 4ê°œë§Œ)
    num_layers = len(self.gpt.gpt_layers)
    for i, block in enumerate(self.gpt.gpt_layers):
      for param in block.parameters():
        param.requires_grad = i >= (num_layers - 4)

  def get_prefix_influence(self, batch_size, seq_len):
    """prefix embeddingsì˜ ì˜í–¥ì„ hidden statesì— ë°˜ì˜"""
    # prefix_embeddingsë¥¼ MLPë¥¼ í†µí•´ ë³€í™˜
    prefix_tokens = self.prefix_embeddings.unsqueeze(0).expand(batch_size, -1, -1)  # [B, prefix_length, prefix_dim]
    prefix_influence = self.prefix_mlp(prefix_tokens)  # [B, prefix_length, hidden_dim]
    
    # í‰ê· ì„ ë‚´ì–´ ì „ì²´ sequenceì— ì ìš©í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜
    prefix_influence = prefix_influence.mean(dim=1, keepdim=True)  # [B, 1, hidden_dim]
    prefix_influence = prefix_influence.expand(-1, seq_len, -1)  # [B, seq_len, hidden_dim]
    
    return prefix_influence * 0.1  # ì‘ì€ ì˜í–¥ë ¥ìœ¼ë¡œ ì‹œì‘

  def forward(self, input_ids, attention_mask, use_prefix=True):
    batch_size = input_ids.size(0)
    
    if use_prefix:
      # ê°„ë‹¨í•œ ë°©ì‹: prefix embeddingsë¥¼ hidden statesì— ì¶”ê°€í•˜ì—¬ ì˜í–¥ ì£¼ê¸°
      outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)
      hidden_states = outputs["last_hidden_state"]  # shape: [batch_size, seq_len, hidden_dim]
      
      # prefix influenceë¥¼ hidden statesì— ì¶”ê°€
      prefix_influence = self.get_prefix_influence(batch_size, hidden_states.size(1))
      hidden_states = hidden_states + prefix_influence
      
    else:
      outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)
      hidden_states = outputs["last_hidden_state"]  # shape: [batch_size, seq_len, hidden_dim]

    # hidden statesë¥¼ vocabulary logitìœ¼ë¡œ ë³€í™˜
    logits = self.gpt.hidden_state_to_token(hidden_states)  # shape: [batch_size, seq_len, vocab_size]
    return logits

  def get_device(self):
    for param in self.gpt.parameters():
      return param.device

  @torch.no_grad()
  def generate(self, encoding, temperature=0.7, top_p=0.9, max_length=128, target_lines=14):
    """Top-p sampling ê¸°ë°˜ 14ì¤„ ì†Œë„¤íŠ¸ ìƒì„± í•¨ìˆ˜ (Prefix-Tuning ì ìš©)"""
    input_ids = encoding.to(self.get_device())
    generated = input_ids.clone()
    
    # êµ¬ë‘ì  í† í° IDë“¤ (ë¬¸ì¥ ì¢…ë£Œë¥¼ ìœ„í•œ)
    punctuation_tokens = {
        self.tokenizer.encode('.')[0]: 2.0,    # ë§ˆì¹¨í‘œ
        self.tokenizer.encode(',')[0]: 1.5,    # ì‰¼í‘œ  
        self.tokenizer.encode(';')[0]: 1.5,    # ì„¸ë¯¸ì½œë¡ 
        self.tokenizer.encode(':')[0]: 1.3,    # ì½œë¡ 
        self.tokenizer.encode('!')[0]: 1.8,    # ëŠë‚Œí‘œ
        self.tokenizer.encode('?')[0]: 1.8,    # ë¬¼ìŒí‘œ
    }

    for step in range(max_length):
        # í˜„ì¬ ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        decoded_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)
        current_lines = decoded_text.count('\n')

        # ğŸ¯ 14ì¤„ ì´ìƒì´ë©´ ì¤‘ë‹¨
        if current_lines >= target_lines:
            break

        # ëª¨ë¸ forward (prefix ì‚¬ìš©)
        attention_mask = torch.ones_like(generated).to(self.get_device())
        logits = self.forward(generated, attention_mask, use_prefix=True)[:, -1, :]  # [B, vocab]

        # ë¬¸ì¥ ê¸¸ì´ ì œì–´: í˜„ì¬ ì¤„ì˜ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
        current_line_text = decoded_text.split('\n')[-1] if '\n' in decoded_text else decoded_text
        current_line_words = len(current_line_text.split())
        
        # ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ êµ¬ë‘ì  ìƒì„±ì„ ì¥ë ¤
        if current_line_words > 8:  # 8ë‹¨ì–´ ì´ìƒì´ë©´ êµ¬ë‘ì  ì¥ë ¤
            punctuation_boost = min(2.0, (current_line_words - 8) * 0.3)  # ìµœëŒ€ 2ë°°ê¹Œì§€
            for token_id, base_boost in punctuation_tokens.items():
                if token_id < logits.size(-1):
                    logits[0, token_id] += punctuation_boost * base_boost
        
        # ë§¤ìš° ê¸´ ë¬¸ì¥ì˜ ê²½ìš° ì¤„ë°”ê¿ˆ ê°•ì œ ì¥ë ¤
        if current_line_words > 12:
            newline_token = self.tokenizer.encode('\n')[0] if self.tokenizer.encode('\n') else None
            if newline_token and newline_token < logits.size(-1):
                logits[0, newline_token] += 3.0

        # Temperature ì ìš© ë° numerical stability ë³´ì¥
        logits = logits / max(temperature, 1e-8)
        
        # Top-p ìƒ˜í”Œë§ (ìˆ˜ì •ëœ ë²„ì „)
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        probs = torch.softmax(sorted_logits, dim=-1)
        cumulative_probs = probs.cumsum(dim=-1)

        # Top-p í•„í„°ë§
        sorted_mask = cumulative_probs > top_p
        # ìµœì†Œí•œ í•˜ë‚˜ì˜ í† í°ì€ ìœ ì§€í•˜ë„ë¡ ë³´ì¥
        if sorted_mask.all():
            sorted_mask[0] = False
        
        # ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ë¥¼ ë§¤ìš° ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì • (ì™„ì „íˆ -infë¡œ í•˜ì§€ ì•ŠìŒ)
        sorted_logits = sorted_logits.clone()
        sorted_logits[sorted_mask] = sorted_logits[sorted_mask] - 1e10
        
        # ë‹¤ì‹œ softmax ì ìš©
        probs = torch.softmax(sorted_logits, dim=-1)
        
        # Numerical stability ì²´í¬
        if torch.isnan(probs).any() or torch.isinf(probs).any() or (probs < 0).any():
            # ë¬¸ì œê°€ ìˆìœ¼ë©´ uniform distributionìœ¼ë¡œ fallback
            probs = torch.ones_like(probs) / probs.size(-1)
        
        # í™•ë¥  ì •ê·œí™” (í˜¹ì‹œ ëª¨ë¥¼ ë¬¸ì œ ë°©ì§€)
        probs = probs / probs.sum(dim=-1, keepdim=True)
        
        # ìƒ˜í”Œë§
        try:
            next_token = torch.multinomial(probs, num_samples=1)
            next_token_id = sorted_indices.gather(-1, next_token)  # shape: [1, 1]
        except RuntimeError:
            # multinomialì´ ì‹¤íŒ¨í•˜ë©´ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í° ì„ íƒ
            next_token = torch.argmax(probs, dim=-1, keepdim=True)
            next_token_id = sorted_indices.gather(-1, next_token)

        # ë‹¤ìŒ í† í° ë¶™ì´ê¸°
        generated = torch.cat([generated, next_token_id], dim=1)

    # ë””ì½”ë”© í›„ ì •í™•íˆ 14ì¤„ë¡œ ì œí•œ
    decoded_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)
    lines = decoded_text.strip().split('\n')
    trimmed_text = '\n'.join(lines[:target_lines])

    return generated, trimmed_text


def save_model(model, optimizer, args, filepath):
  save_info = {
    'model': model.state_dict(),
    'optim': optimizer.state_dict(),
    'args': args,
    'system_rng': random.getstate(),
    'numpy_rng': np.random.get_state(),
    'torch_rng': torch.random.get_rng_state(),
  }

  torch.save(save_info, filepath)
  print(f"save the model to {filepath}")


def train(args):
  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')
  sonnet_dataset = SonnetsDataset(args.sonnet_path)
  sonnet_dataloader = DataLoader(sonnet_dataset, shuffle=True, batch_size=args.batch_size,
                                 collate_fn=sonnet_dataset.collate_fn)
  held_out_sonnet_dataset = SonnetsDataset(args.held_out_sonnet_path)

  args = add_arguments(args)
  model = SonnetGPT(args).to(device)
  
  # Prefix-Tuning: prefix parametersë§Œ í•™ìŠµí•˜ê³  GPT íŒŒë¼ë¯¸í„°ëŠ” ê³ ì •
  if args.prefix_only:
    # GPT íŒŒë¼ë¯¸í„° ê³ ì •
    for param in model.gpt.parameters():
      param.requires_grad = False
    
    # Prefix ê´€ë ¨ íŒŒë¼ë¯¸í„°ë§Œ í•™ìŠµ
    trainable_params = [model.prefix_embeddings]
    for param in model.prefix_mlp.parameters():
      trainable_params.append(param)
    
    optimizer = AdamW(trainable_params, lr=args.lr)
    print("Training only prefix parameters (Prefix-Tuning mode)")
  else:
    # ê¸°ì¡´ ë°©ì‹: ì¼ë¶€ ë ˆì´ì–´ + prefix parameters í•™ìŠµ
    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr)
    print("Training last layers + prefix parameters (Hybrid mode)")

  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5)

  best_loss = float('inf')
  patience = 3
  counter = 0

  for epoch in range(args.epochs):
    model.train()
    train_loss = 0
    num_batches = 0

    for batch in tqdm(sonnet_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE):
        b_ids, b_mask = batch['token_ids'].to(device), batch['attention_mask'].to(device)
        optimizer.zero_grad()
        
        # Prefix-Tuning ì ìš©
        logits = model(b_ids, b_mask, use_prefix=True)
        
        # Loss ê³„ì‚° ì‹œ prefix ë¶€ë¶„ ì œì™¸
        logits = rearrange(logits[:, :-1], 'b t d -> (b t) d')
        labels = b_ids[:, 1:].contiguous().flatten()
        loss = F.cross_entropy(logits, labels, reduction='mean')
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        num_batches += 1

    train_loss = train_loss / num_batches
    scheduler.step(train_loss)
    print(f"Epoch {epoch}: train loss :: {train_loss :.3f}.")

    if train_loss < best_loss:
        best_loss = train_loss
        counter = 0
        print(f"Saving best model at epoch {epoch} with loss {train_loss:.3f}")
        save_model(model, optimizer, args, f'best_{args.filepath}')
    else:
        counter += 1
        if counter >= patience:
            print("Early stopping triggered.")
            break
        
  print("Training completed!")
  return model

@torch.no_grad()
def generate_submission_sonnets(args):
  device = torch.device('cuda') if args.use_gpu else torch.device('cpu')
  # ìµœì¢… í›ˆë ¨ëœ ëª¨ë¸ ë˜ëŠ” best ëª¨ë¸ ë¡œë“œ
  try:
    saved = torch.load(f'best_{args.filepath}', weights_only=False)
    print("Loading best model for generation...")
  except FileNotFoundError:
    saved = torch.load(f'{args.epochs-1}_{args.filepath}', weights_only=False)
    print("Loading final epoch model for generation...")
  
  model = SonnetGPT(saved['args'])
  model.load_state_dict(saved['model'])
  model = model.to(device).eval()

  held_out_sonnet_dataset = SonnetsDataset(args.held_out_sonnet_path)
  generated_sonnets = []

  print("Generating submission sonnets with Prefix-Tuning...")
  for batch in held_out_sonnet_dataset:
    sonnet_id = batch[0]
    encoding = model.tokenizer(batch[1], return_tensors='pt', padding=False, truncation=True)
    encoding = {k: v.to(device) for k, v in encoding.items()}
    output = model.generate(encoding['input_ids'], temperature=args.temperature, top_p=args.top_p)
    decoded_output = model.tokenizer.decode(output[0][0])
    generated_sonnets.append((sonnet_id, f'{decoded_output}\n\n'))

  with open(args.sonnet_out, "w+") as f:
    f.write(f"--Generated Sonnets (with Prefix-Tuning)-- \n\n")
    for sonnet in generated_sonnets:
      f.write(f"\n{sonnet[0]}\n")
      f.write(sonnet[1])
  
  print(f"Generated sonnets saved to {args.sonnet_out}")


def get_args():
  parser = argparse.ArgumentParser()
  parser.add_argument("--sonnet_path", type=str, default="data/sonnets.txt")
  parser.add_argument("--held_out_sonnet_path", type=str, default="data/sonnets_held_out.txt")
  parser.add_argument("--sonnet_out", type=str, default="predictions/generated_sonnets_B.txt")
  parser.add_argument("--seed", type=int, default=11711)
  parser.add_argument("--epochs", type=int, default=10)
  parser.add_argument("--use_gpu", action='store_true')
  parser.add_argument("--temperature", type=float, default=0.8)
  parser.add_argument("--top_p", type=float, default=0.9)
  parser.add_argument("--batch_size", type=int, default=8)
  parser.add_argument("--lr", type=float, default=1e-5)
  parser.add_argument("--model_size", type=str, choices=['gpt2', 'gpt2-medium', 'gpt2-large'], default='gpt2')
  
  # Prefix-Tuning ê´€ë ¨ íŒŒë¼ë¯¸í„°
  parser.add_argument("--prefix_length", type=int, default=10, help="Length of prefix tokens")
  parser.add_argument("--prefix_only", action='store_true', help="Train only prefix parameters (freeze GPT)")
  
  return parser.parse_args()

def add_arguments(args):
  if args.model_size == 'gpt2':
    args.d = 768
    args.l = 12
    args.num_heads = 12
  elif args.model_size == 'gpt2-medium':
    args.d = 1024
    args.l = 24
    args.num_heads = 16
  elif args.model_size == 'gpt2-large':
    args.d = 1280
    args.l = 36
    args.num_heads = 20
  else:
    raise Exception(f'{args.model_size} is not supported.')
  return args

if __name__ == "__main__":
  args = get_args()
  args.filepath = f'{args.epochs}-{args.lr}-sonnet-prefix_B.pt'
  seed_everything(args.seed)
  
  # í›ˆë ¨ ì‹¤í–‰
  trained_model = train(args)
  
  # í›ˆë ¨ ì™„ë£Œ í›„ ìµœì¢… ì†Œë„¤íŠ¸ ìƒì„±
  print("\n" + "="*50)
  print("Training completed! Generating final submission sonnets...")
  print("="*50)
  generate_submission_sonnets(args)