# π§  GPT-2 κΈ°λ° μμ—°μ–΄μ²λ¦¬ νƒμ¤ν¬ μ‹¤ν— ν”„λ΅μ νΈ

λ³Έ ν”„λ΅μ νΈλ” OpenAIμ GPT-2 κµ¬μ΅°λ¥Ό κΈ°λ°μΌλ΅ μ§μ ‘ λ¨λΈμ„ κµ¬ν„ν•κ³ , λ‹¤μ–‘ν• Fine-Tuning μ „λµμ„ μ μ©ν•μ—¬ **κ°μ • λ¶„μ„**, **ν¨λ¬ν”„λ μ΄μ¦ νƒμ§€**, **μ‹ μƒμ„±** μ„Έ κ°€μ§€ μμ—°μ–΄μ²λ¦¬(NLP) νƒμ¤ν¬μ— μ μ©ν• μ‹¤ν—μ„ μν–‰ν•μ€μµλ‹λ‹¤.

μ£Όμ” λ©ν‘λ” GPT-2 decoder-only κµ¬μ΅°μ νΉμ„±μ„ μ΄ν•΄ν•κ³ , μ΄λ¥Ό λ‹¤μ–‘ν• downstream taskμ— ν¨κ³Όμ μΌλ΅ ν™μ©ν•κΈ° μ„ν• fine-tuning μ „λµμ„ μ„¤κ³„Β·λΉ„κµΒ·λ¶„μ„ν•λ” κ²ƒμ…λ‹λ‹¤.

---

## π—‚ ν”„λ΅μ νΈ κµ¬μ΅°

```

π“ data               β†’ μ „μ²λ¦¬λ λ°μ΄ν„°μ…‹(csv) μ €μ¥ ν΄λ”
π“ models             β†’ λ¨λΈ μ •μ νμΌ (e.g., GPT2Model)
π“ modules            β†’ Attention, Transformer block λ“± ν•μ„ λ¨λ“ μ •μ
π“ predictions        β†’ μμΈ΅ κ²°κ³Ό μ €μ¥ ν΄λ” (csv μ¶λ ¥λ¨)

π“„ classifier\_baseline.py   β†’ Full fine-tuning κΈ°λ° κ°μ •λ¶„μ„ λ¨λΈ μ‹¤ν–‰ νμΌ (SST, CFIMDB)
π“„ classifier\_taskA.py      β†’ ULMFiT μ „λµ μ μ© λ¨λΈ μ‹¤ν–‰ νμΌ
π“„ config.py                β†’ λ¨λΈ μ„¤μ • λ° νλΌλ―Έν„° κ΄€λ¦¬
π“„ datasets.py              β†’ Dataset ν΄λμ¤ λ° tokenizer μ²λ¦¬ μ •μ
π“„ env.yml                  β†’ Conda ν™κ²½ μ„¤μ • νμΌ
π“„ README.md                β†’ μ‹¤ν–‰ λ° μ‹¤ν— μ•λ‚΄ νμΌ

````

---

## π§ μ‹¤ν— ν™κ²½

- **ν™κ²½**: Google Colab (T4 GPU μ‚¬μ©)
- **ν”„λ μ„μ›ν¬**: PyTorch, Huggingface Transformers
- **Python λ²„μ „**: 3.8
- **ν›λ ¨ μ‹κ°„**:
  - SST-5 (Baseline): μ•½ 25λ¶„
  - SST-5 (ULMFiT): μ•½ 20λ¶„
  - CFIMDB: μ•½ 15λ¶„

---

## π€ μ‹¤ν–‰ λ°©λ²•

### 1. ν™κ²½ μ„Έν…
```bash
# Conda ν™κ²½ μƒμ„±
conda env create -f env.yml
conda activate gpt2-nlp
````

### 2. λ² μ΄μ¤λΌμΈ κ°μ •λ¶„μ„ μ‹¤ν–‰ (SST + CFIMDB)

```bash
python classifier_baseline.py --fine-tune-mode full-model --use_gpu
```

### 3. ULMFiT μ „λµ μ‹¤ν— (Task A)

```bash
python classifier_taskA.py --fine-tune-mode full-model --use_gpu
```


## π“ μ£Όμ” κ²°κ³Ό μ”μ•½

| λ¨λΈ                 | λ°μ΄ν„°μ…‹   | Dev Accuracy | Dev F1 Score |
| ------------------ | ------ | ------------ | ------------ |
| Baseline (Full FT) | SST-5  | 51.8%        | 49.0%        |
| Task A (ULMFiT)    | SST-5  | 50.9%        | 48.0%        |
| Baseline (Full FT) | CFIMDB | 98.8%        | 98.8%        |

---

## π“ μ°Έκ³  μ‚¬ν•­

* ν•™μµ μ¤‘κ°„ μ²΄ν¬ν¬μΈνΈ μ €μ¥μ€ `.pt` νμΌλ΅ κµ¬ν„λμ–΄ μμΌλ©°, ν•„μ” μ‹ `torch.load()`λ΅ λ¶λ¬μ™€ ν‰κ°€ν•  μ μμµλ‹λ‹¤.
* λ¨λ“  μ‹¤ν— κ²°κ³Όλ” reproducibilityλ¥Ό μ„ν•΄ `random seed = 11711`λ΅ κ³ μ •ν•μ€μµλ‹λ‹¤.
* λ¨λΈ κµ¬μ΅°λ” `models/gpt2.py`μ™€ `modules/attention.py` λ“±μΌλ΅ λ¨λ“ν™”λμ–΄ μμ–΄ ν™•μ¥ κ°€λ¥ν•λ„λ΅ κµ¬μ„±λμ–΄ μμµλ‹λ‹¤.

---

## π‘©β€π’» Contributors

* [Jihyeon0604](https://github.com/Jihyeon0604)
* [2rayjja](https://github.com/2rayjja)
* [ha-hyeon](https://github.com/ha-hyeon)

---

