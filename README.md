# ğŸ§  GPT-2 ê¸°ë°˜ ìì—°ì–´ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹¤í—˜ í”„ë¡œì íŠ¸

ë³¸ í”„ë¡œì íŠ¸ëŠ” OpenAIì˜ GPT-2 êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ì ‘ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³ , ë‹¤ì–‘í•œ Fine-Tuning ì „ëµì„ ì ìš©í•˜ì—¬ **ê°ì • ë¶„ì„**, **íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ íƒì§€**, **ì‹œ ìƒì„±** ì„¸ ê°€ì§€ ìì—°ì–´ì²˜ë¦¬(NLP) íƒœìŠ¤í¬ì— ì ìš©í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

ì£¼ìš” ëª©í‘œëŠ” GPT-2 decoder-only êµ¬ì¡°ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ë‹¤ì–‘í•œ downstream taskì— íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ê¸° ìœ„í•œ fine-tuning ì „ëµì„ ì„¤ê³„Â·ë¹„êµÂ·ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

---

## ğŸ—‚ ìˆ˜ì •ëœ í”„ë¡œì íŠ¸ íŒŒì¼

```

ğŸ“„ classifier\_baseline.py   â†’ Full fine-tuning ê¸°ë°˜ ê°ì •ë¶„ì„ ëª¨ë¸ ì‹¤í–‰ íŒŒì¼ (SST, CFIMDB)
ğŸ“„ classifier\_taskA.py      â†’ ULMFiT ì „ëµ ì ìš© ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_generation_baseline.py   â†’ fine-tuning ê¸°ë°˜ ê¸°ë³¸ ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_generation_taskA.py      â†’ Unlikelihood Loss ì ìš© ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_generation_taskB.py      â†’ Prefix Tuning ì ìš© ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_generation_taskC.py      â†’ Unlikelihood Loss + Prefix Tuning ì ìš© ëª¨ë¸ ì‹¤í–‰ íŒŒì¼


````

---

## ğŸ§ª ì‹¤í—˜ í™˜ê²½

- **í”„ë ˆì„ì›Œí¬**: PyTorch, Huggingface Transformers
- **Python ë²„ì „**: 3.8
- **í™˜ê²½**:
  - ê°ì •ë¶„ì„ : Google Colab (T4 GPU ì‚¬ìš©)
  - ì‹œ ìƒì„± : NVIDIA TITAN RTX 1ëŒ€  
- **í›ˆë ¨ ì‹œê°„**:
  - ê°ì •ë¶„ì„ :
    - SST-5 (Baseline): ì•½ 25ë¶„
    - SST-5 (ULMFiT): ì•½ 20ë¶„
    - CFIMDB: ì•½ 15ë¶„
  - ì‹œ ìƒì„± :
    - Baseline: Epochë‹¹ ì•½ 1ì´ˆ
    - Task A (Unlikelihood Loss): Epochë‹¹ ì•½ 7ì´ˆ
    - Task B (Prefix Tuning): Epochë‹¹ ì•½ 1ì´ˆ
    - Task C (A+B í˜¼í•©): Epochë‹¹ ì•½ 7ì´ˆ

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. í™˜ê²½ ì„¸íŒ…
```bash
# Conda í™˜ê²½ ìƒì„±
conda env create -f env.yml
conda activate gpt2-nlp
````

### 2. ê°ì •ë¶„ì„ ë² ì´ìŠ¤ë¼ì¸ ì‹¤í–‰ (SST + CFIMDB)

```bash
python classifier_baseline.py --fine-tune-mode full-model --use_gpu
```

### 3. ê°ì •ë¶„ì„ ULMFiT ì „ëµ ì‹¤í—˜ (Task A)

```bash
python classifier_taskA.py --fine-tune-mode full-model --use_gpu
```

### 6. ì‹œ ìƒì„± ë² ì´ìŠ¤ë¼ì¸ ì‹¤í–‰

```bash
python sonnet_generation_baseline.py --use_gpu
```

### 7. ì‹œ ìƒì„± ì „ëµ ì‹¤í—˜ (Task A, B, C)

```bash
python sonnet_generation_taskA.py --use_gpu
python sonnet_generation_taskB.py --use_gpu
python sonnet_generation_taskC.py --use_gpu
```

## ğŸ“Š ì£¼ìš” ê²°ê³¼ ìš”ì•½
### ê°ì •ë¶„ì„ ì„±ëŠ¥ì§€í‘œ

| ëª¨ë¸                 | ë°ì´í„°ì…‹   | Dev Accuracy | Dev F1 Score |
| ------------------ | ------ | ------------ | ------------ |
| Baseline (Full FT) | SST-5  | 51.8%        | 49.0%        |
| Task A (ULMFiT)    | SST-5  | 50.9%        | 48.0%        |
| Baseline (Full FT) | CFIMDB | 98.8%        | 98.8%        |

### ì‹œ ìƒì„± ì„±ëŠ¥ì§€í‘œ
| ì „   | Perplexity â†“ | Distinct-1 â†‘ | Distinct-2 â†‘ | Rhyming Accuracy â†‘ |
| ---- | ------------ | ------------ | ------------ | ------------------ |
| Base | 55.73        | 0.621        | 0.927        | 0.141              |
| A    | 51.68        | 0.613        | 0.919        | **0.215**          |
| B    | **50.11**    | 0.572        | 0.878        | 0.166              |
| C    | 55.70        | **0.641**    | **0.944**    | 0.132              |

ğŸ“Œ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
**Task A (Unlikelihood Loss)**ëŠ” ê°€ì¥ ê· í˜• ì¡íŒ ì„±ëŠ¥ì„ ë³´ì„:

Perplexity ê°œì„  (55.73 â†’ 51.68)

Rhyming Accuracy í–¥ìƒ (0.141 â†’ 0.215)

Distinct ì§€í‘œ ì†Œí­ í•˜ë½í–ˆìœ¼ë‚˜ ì—¬ì „íˆ ìš°ìˆ˜í•œ ë‹¤ì–‘ì„± ìœ ì§€

Task B (Prefix Tuning):

ê°€ì¥ ë‚®ì€ Perplexity ê¸°ë¡ (50.11)

í•˜ì§€ë§Œ ë‹¤ì–‘ì„±(Distinct-1/2) ì €í•˜

Task C (Unlikelihood + Prefix):

ê°€ì¥ ë†’ì€ ë‹¤ì–‘ì„±(Distinct-1: 0.641, Distinct-2: 0.944)

í•˜ì§€ë§Œ Rhyming Accuracyê°€ ê°€ì¥ ë‚®ìŒ (0.132)

---

## ğŸ“Œ ì°¸ê³  ì‚¬í•­

* í•™ìŠµ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì€ `.pt` íŒŒì¼ë¡œ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë©°, í•„ìš” ì‹œ `torch.load()`ë¡œ ë¶ˆëŸ¬ì™€ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ëŠ” reproducibilityë¥¼ ìœ„í•´ `random seed = 11711`ë¡œ ê³ ì •í•˜ì˜€ìŠµë‹ˆë‹¤.
* ëª¨ë¸ êµ¬ì¡°ëŠ” `models/gpt2.py`ì™€ `modules/attention.py` ë“±ìœ¼ë¡œ ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´ í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ‘©â€ğŸ’» Contributors

* [Jihyeon0604](https://github.com/Jihyeon0604)
* [2rayjja](https://github.com/2rayija)
* [ha-hyeon](https://github.com/ha-hyeon)

---

