# ğŸ§  GPT-2 ê¸°ë°˜ ìì—°ì–´ì²˜ë¦¬ íƒœìŠ¤í¬ ì‹¤í—˜ í”„ë¡œì íŠ¸

ë³¸ í”„ë¡œì íŠ¸ëŠ” OpenAIì˜ GPT-2 êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ì ‘ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³ , ë‹¤ì–‘í•œ Fine-Tuning ì „ëµì„ ì ìš©í•˜ì—¬ **ê°ì • ë¶„ì„**, **íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ íƒì§€**, **ì‹œ ìƒì„±** ì„¸ ê°€ì§€ ìì—°ì–´ì²˜ë¦¬(NLP) íƒœìŠ¤í¬ì— ì ìš©í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì˜€ìŠµë‹ˆë‹¤.

ì£¼ìš” ëª©í‘œëŠ” GPT-2 decoder-only êµ¬ì¡°ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ê³ , ì´ë¥¼ ë‹¤ì–‘í•œ downstream taskì— íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ê¸° ìœ„í•œ fine-tuning ì „ëµì„ ì„¤ê³„Â·ë¹„êµÂ·ë¶„ì„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

---

## ğŸ—‚ ìˆ˜ì •ëœ í”„ë¡œì íŠ¸ íŒŒì¼

```

ğŸ“„ classifier\_baseline.py   â†’ Full fine-tuning ê¸°ë°˜ ê°ì •ë¶„ì„ ëª¨ë¸ ì‹¤í–‰ íŒŒì¼ (SST, CFIMDB)
ğŸ“„ classifier\_taskA.py      â†’ ULMFiT ì „ëµ ì ìš© ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_generation_baseline.py   â†’ fine-tuning ê¸°ë°˜ ê¸°ë³¸ ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_generation_taskA.py      â†’ Unlikelihood Loss ì ìš© ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_generation_taskB.py      â†’ Prefix Tuning ì ìš© ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_generation_taskC.py      â†’ Unlikelihood Loss + Prefix Tuning ì ìš© ëª¨ë¸ ì‹¤í–‰ íŒŒì¼
ğŸ“„ sonnet_eval.py                  â†’ ìƒì„±ëœ ì†Œë„· í‰ê°€ íŒŒì¼
ğŸ“„ paraphrase_detection.py         â†’ paraphrase_detection baseline íŒŒì¼
ğŸ“„ paraphrase_detection_taskA.py   â†’ Last Hidden ë° Standard Dropout ê¸°ë²• ì ìš© íŒŒì¼
ğŸ“„ paraphrase_detection_taskB.py   â†’ Mean Pooling ë° Multi-Sample Dropout ê¸°ë²• ì ìš© íŒŒì¼
ğŸ“„ paraphrase_detection_taskC.py   â†’ Hybrid(Mean + Last Hidden) ë° Dynamic Dropout ê¸°ë²• ì ìš© íŒŒì¼
ğŸ“„ predictions/para-dev-output.csv        â†’ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì˜ Dev ì…‹ ì˜ˆì¸¡ ê²°ê³¼
ğŸ“„ predictions/para-dev-output_taskA.csv  â†’ Task A ëª¨ë¸ì˜ Dev ì…‹ ì˜ˆì¸¡ ê²°ê³¼
ğŸ“„ predictions/para-dev-output_taskB.csv  â†’ Task B ëª¨ë¸ì˜ Dev ì…‹ ì˜ˆì¸¡ ê²°ê³¼
ğŸ“„ predictions/para-dev-output_taskC.csv  â†’ Task C ëª¨ë¸ì˜ Dev ì…‹ ì˜ˆì¸¡ ê²°ê³¼
ğŸ“„ predictions/para-test-output.csv       â†’ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì˜ Test ì…‹ ì˜ˆì¸¡ ê²°ê³¼
ğŸ“„ predictions/para-test-output_taskA.csv â†’ Task A ëª¨ë¸ì˜ Test ì…‹ ì˜ˆì¸¡ ê²°ê³¼
ğŸ“„ predictions/para-test-output_taskB.csv â†’ Task B ëª¨ë¸ì˜ Test ì…‹ ì˜ˆì¸¡ ê²°ê³¼
ğŸ“„ predictions/para-test-output_taskC.csv â†’ Task C ëª¨ë¸ì˜ Test ì…‹ ì˜ˆì¸¡ ê²°ê³¼

````

---

## ğŸ§ª ì‹¤í—˜ í™˜ê²½

- **í”„ë ˆì„ì›Œí¬**: PyTorch, Huggingface Transformers
- **Python ë²„ì „**: 3.8
- **í™˜ê²½**:
  - ê°ì •ë¶„ì„ : Google Colab (T4 GPU ì‚¬ìš©)
  - paraphrase detection : Google Colab (L4 GPU ì‚¬ìš©)
  - ì‹œ ìƒì„± : NVIDIA TITAN RTX 1ëŒ€  
- **í›ˆë ¨ ì‹œê°„**:
  - ê°ì •ë¶„ì„ :
    - SST-5 (Baseline): ì•½ 25ë¶„
    - SST-5 (ULMFiT): ì•½ 20ë¶„
    - CFIMDB: ì•½ 15ë¶„
  - paraphrase detection :
    - Baseline: Epochë‹¹ ì•½ 46ë¶„ 
    - Task A (Last Hidden ë° Standard Dropout): Epochë‹¹ ì•½ 39ë¶„
    - Task B (Mean Pooling ë° Multi-Sample Dropout): Epochë‹¹ ì•½ 39ë¶„
    - Task C (Hybrid(Mean + Last Hidden) ë° Dynamic Dropout ): Epochë‹¹ ì•½ 39ë¶„
  - ì‹œ ìƒì„± :
    - Baseline: Epochë‹¹ ì•½ 1ì´ˆ
    - Task A (Unlikelihood Loss): Epochë‹¹ ì•½ 7ì´ˆ
    - Task B (Prefix Tuning): Epochë‹¹ ì•½ 1ì´ˆ
    - Task C (A+B í˜¼í•©): Epochë‹¹ ì•½ 7ì´ˆ

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. í™˜ê²½ ì„¸íŒ…
```bash
# Conda í™˜ê²½ ìƒì„±
conda env create -f env.yml
conda activate gpt2-nlp
````

### 2. ê°ì •ë¶„ì„ ë² ì´ìŠ¤ë¼ì¸ ì‹¤í–‰ (SST + CFIMDB)

```bash
python classifier_baseline.py --fine-tune-mode full-model --use_gpu
```

### 3. ê°ì •ë¶„ì„ ULMFiT ì „ëµ ì‹¤í—˜ (Task A)

```bash
python classifier_taskA.py --fine-tune-mode full-model --use_gpu
```

### 4. Paraphrase detection ë² ì´ìŠ¤ë¼ì¸ ì‹¤í—˜ (Baseline)

```bash
python python paraphrase_detection.py --use_gpu
```

### ì½”ë©ìš©
```bash
from google.colab import drive
drive.mount('/content/drive')
!git clone https://github.com/Jihyeon0604/nlp_gpt2.git
%cd nlp_gpt2
!pip install -r requirements.txt
!pip install sacrebleu
!pip install huggingface_hub[hf_xet]
!python paraphrase_detection.py --use_gpu
# ê²½ë¡œ ì§€ì •
!mkdir -p /content/drive/MyDrive/
# ì €ì¥
# íŒŒì¼ ë³µì‚¬í•˜ë©´ì„œ ì´ë¦„ ë°”ê¾¸ê¸°
!cp predictions/para-dev-output.csv /content/drive/MyDrive/para-dev-output.csv.csv
!cp predictions/para-test-output.csv /content/drive/MyDrive/para-test-output.csv
!cp 10-1e-05-paraphrase_task.pt /content/drive/MyDrive/10-1e-05-paraphrase_task.pt
```

### 5. Paraphrase detection ì‹¤í—˜ (Task A,B,C)

```bash
python python paraphrase_detection_taskA.py --use_gpu
python python paraphrase_detection_taskB.py --use_gpu
python python paraphrase_detection_taskC.py --use_gpu
```

### 6. ì‹œ ìƒì„± ë² ì´ìŠ¤ë¼ì¸ ì‹¤í–‰

```bash
python sonnet_generation_baseline.py --use_gpu
```

### 7. ì‹œ ìƒì„± ì „ëµ ì‹¤í—˜ (Task A, B, C)

```bash
python sonnet_generation_taskA.py --use_gpu
python sonnet_generation_taskB.py --use_gpu
python sonnet_generation_taskC.py --use_gpu
```

### 8. ì‹œ ìƒì„± ì €ì¥ ê²½ë¡œ ë° í‰ê°€ ë°©ë²•
#### sonnet_generation_baseline.py ì‹¤í–‰ ì‹œ
ê°€ì¤‘ì¹˜ëŠ” args.filepath (default = f'{args.epochs}-{args.lr}-sonnet.pt')ì— ì €ì¥\
ìƒì„±ëœ ì†Œë„·ì€ args.sonnet_out (default = predictions/generated_sonnets.txt)ì— ì €ì¥

#### sonnet_generation_taskA.py ì‹¤í–‰ ì‹œ
ê°€ì¤‘ì¹˜ëŠ” f'{args.epochs}-{args.lr}-sonnet_A.pt'ì— ì €ì¥\
ìƒì„±ëœ ì†Œë„·ì€ predictions/generated_sonnets_A.txtì— ì €ì¥

#### sonnet_generation_taskB.py ì‹¤í–‰ ì‹œ
ê°€ì¤‘ì¹˜ëŠ” f'{args.epochs}-{args.lr}-sonnet_B.pt'ì— ì €ì¥\
ìƒì„±ëœ ì†Œë„·ì€ predictions/generated_sonnets_B.txtì— ì €ì¥ 

#### sonnet_generation_taskC.py ì‹¤í–‰ ì‹œ
ê°€ì¤‘ì¹˜ëŠ” f'{args.epochs}-{args.lr}-sonnet_C.pt'ì— ì €ì¥\
ìƒì„±ëœ ì†Œë„·ì€ predictions/generated_sonnets_C.txtì— ì €ì¥

ì´í›„ sonnet_eval.pyë¡œ ì‹¤í–‰ ìƒì„±ëœ ì†Œë„· í‰ê°€ ê°€ëŠ¥
```bash
python sonnet_eval.py
```

sonnet_eval.pyì˜ 161ë²ˆì§¸ ì¤„ filepath = 'predictions/generated_sonnets.txt'ë¥¼ ìœ„ì˜ args.sonnet_outì— ë§ê²Œ ìˆ˜ì •í•œ í›„ sonnet_eval ì‹¤í–‰
```bash
baseline í‰ê°€ ì‹œ filepath = 'predictions/generated_sonnets.txt'
taskA í‰ê°€ ì‹œ filepath = 'predictions/generated_sonnets_A.txt'
taskB í‰ê°€ ì‹œ filepath = 'predictions/generated_sonnets_B.txt'
taskC í‰ê°€ ì‹œ filepath = 'predictions/generated_sonnets_C.txt'
```

## ğŸ“Š ì£¼ìš” ê²°ê³¼ ìš”ì•½

### ê°ì •ë¶„ì„ ì„±ëŠ¥ì§€í‘œ
| ëª¨ë¸                 | ë°ì´í„°ì…‹   | Dev Accuracy | Dev F1 Score |
| ------------------ | ------ | ------------ | ------------ |
| Baseline (Full FT) | SST-5  | 51.8%        | 49.0%        |
| Task A (ULMFiT)    | SST-5  | 50.9%        | 48.0%        |
| Baseline (Full FT) | CFIMDB | 98.8%        | 98.8%        |

### paraphrase detection ì„±ëŠ¥ì§€í‘œ
| ëª¨ë¸                      | ë°ì´í„°ì…‹  | Dev Accuracy | Dev F1 Score |
| ----------------------- | ----- | ------------ | ------------ |
| Baseline (Full FT)      | Quora | **89.9%**    | **89.2%**    |
| Task A (Last Hidden)    | Quora | 89.6%        | -            |
| Task B (Mean + Dropout) | Quora | 89.3%        | 88.7%        |
| Task C (Mixed Pooling)  | Quora | 89.63%       | 88.99%       |

### ì‹œ ìƒì„± ì„±ëŠ¥ì§€í‘œ
| ëª¨ë¸   | Perplexity â†“ | Distinct-1 â†‘ | Distinct-2 â†‘ | Rhyming Accuracy â†‘ |
| -------- | ------------ | ------------ | ------------ | ------------------ |
| Baseline | 55.73        | 0.621        | 0.927        | 0.141              |
| Task A    | 51.68        | 0.613        | 0.919        | **0.215**          |
| Task B    | **50.11**    | 0.572        | 0.878        | 0.166              |
| Task C    | 55.70        | **0.641**    | **0.944**    | 0.132              |

ğŸ“Œ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
**Task A (Unlikelihood Loss)**ëŠ” ê°€ì¥ ê· í˜• ì¡íŒ ì„±ëŠ¥ì„ ë³´ì„:\
Perplexity ê°œì„  (55.73 â†’ 51.68)\
Rhyming Accuracy í–¥ìƒ (0.141 â†’ 0.215)\
Distinct ì§€í‘œ ì†Œí­ í•˜ë½í–ˆìœ¼ë‚˜ ì—¬ì „íˆ ìš°ìˆ˜í•œ ë‹¤ì–‘ì„± ìœ ì§€

Task B (Prefix Tuning):\
ê°€ì¥ ë‚®ì€ Perplexity ê¸°ë¡ (50.11)\
í•˜ì§€ë§Œ ë‹¤ì–‘ì„±(Distinct-1/2) ì €í•˜

Task C (Unlikelihood + Prefix):\
ê°€ì¥ ë†’ì€ ë‹¤ì–‘ì„±(Distinct-1: 0.641, Distinct-2: 0.944)\
í•˜ì§€ë§Œ Rhyming Accuracyê°€ ê°€ì¥ ë‚®ìŒ (0.132)

---

## ğŸ“Œ ì°¸ê³  ì‚¬í•­

* í•™ìŠµ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì€ `.pt` íŒŒì¼ë¡œ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë©°, í•„ìš” ì‹œ `torch.load()`ë¡œ ë¶ˆëŸ¬ì™€ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* ëª¨ë“  ì‹¤í—˜ ê²°ê³¼ëŠ” reproducibilityë¥¼ ìœ„í•´ `random seed = 11711`ë¡œ ê³ ì •í•˜ì˜€ìŠµë‹ˆë‹¤.
* ëª¨ë¸ êµ¬ì¡°ëŠ” `models/gpt2.py`ì™€ `modules/attention.py` ë“±ìœ¼ë¡œ ëª¨ë“ˆí™”ë˜ì–´ ìˆì–´ í™•ì¥ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ‘©â€ğŸ’» Contributors

* [Jihyeon0604](https://github.com/Jihyeon0604)
* [2rayjja](https://github.com/2rayija)
* [ha-hyeon](https://github.com/ha-hyeon)

---

